from __future__ import annotations
import uuid, json
from fastapi import APIRouter, Depends, Header, HTTPException, Query, Request
from fastapi.responses import HTMLResponse
from fastapi.responses import StreamingResponse
from typing import Optional
from .schemas import TaskV11, TaskStatus, FeedbackV1
from .settings import settings
from .sse import StreamHub
from .db import get_engine, insert_task, update_task_status, get_task
from sqlalchemy import text
from .registry import available_models
from .bandit import extract_features, feature_hash, get_stats_for_models, estimate_mean
from .ollama_health import get_ollama_health
from .ratelimit import check_allow, peek_state
from .logging_setup import get_logger

router = APIRouter()
log = get_logger("api")
hub: StreamHub = StreamHub()
job_queue = None  # set in main


def _ratelimit_guard(x_api_key: str|None):
    key = x_api_key or "anon"
    ok, retry_ms = check_allow(key)
    if not ok:
        raise HTTPException(status_code=429, detail=f"rate limit exceeded; retry in {retry_ms}ms")


def require_api_key(x_api_key: Optional[str] = Header(None)):
    if x_api_key != settings.api_key:
        raise HTTPException(status_code=401, detail="Invalid or missing API key")

@router.get("/health")
async def health():
    return {"ok": True}

@router.get("/v1/ollama/health")
async def ollama_health():
    return await get_ollama_health()

@router.get("/v1/ratelimit/check")
async def ratelimit_check(x_api_key: str | None = Header(None), consume: int = 0):
    key = x_api_key or "anon"
    if consume:
        from .ratelimit import check_allow
        ok, retry_ms = check_allow(key)
        status = "allowed" if ok else f"blocked (retry_in_ms={retry_ms})"
    else:
        status = "peek"
    tokens, last, rps, burst = peek_state(key)
    return {"key": key, "mode": status, "tokens": round(tokens, 3), "rps": rps, "burst": burst}

@router.get("/v1/models")
async def list_models(language: Optional[str] = None, debug: int = Query(0, ge=0, le=1)):
    models = available_models(language)
    if not debug:
        return {"models": models}
    # attach bandit mean estimates for a default feature context (based on provided language)
    dummy_job = {
        "input": {
            "language": language or "any",
            "frameworks": [],
            "repo": {"path": "./workspace", "include": [], "exclude": []},
            "constraints": {"max_tokens": 2048},
            "goal": ""
        },
        "output_contract": {"expected_files": []}
    }
    feats = extract_features(dummy_job)
    fh = feature_hash(feats)
    full_names = [f"{m.get('name')}:{(m.get('size') if str(m.get('size','')).endswith('b') else (str(m.get('size',''))+'b' if m.get('size') else ''))}-{m.get('quant','')}".strip("-") for m in models]
    eng = await get_engine()
    async with eng.connect() as conn:
        stats = await get_stats_for_models(conn, full_names, fh)
    enriched = []
    for m, name in zip(models, full_names):
        runs, rs = stats.get(name, (0, 0.0))
        enriched.append({**m, "_bandit":{"runs":runs, "mean_estimate": round(estimate_mean(runs, rs), 3)}})
    return {"models": enriched, "_feature_hash": fh, "_features": feats}

@router.post("/v1/tasks", dependencies=[Depends(require_api_key)])
async def submit_task(task: TaskV11, request: Request, x_api_key: str | None = Header(None)):
    eng = await get_engine()
    async with eng.begin() as conn:
        await insert_task(conn, task.id, task.type, task.input.language, "queued", task.prompt_template_version)
    # robust queue lookup: module global or app.state
    q = job_queue or getattr(request.app.state, 'job_queue', None)
    if q is None:
        raise HTTPException(status_code=503, detail='job queue not ready')
    await q.submit(task.model_dump())
    return {"task_id": str(task.id)}

@router.get("/v1/tasks/{task_id}")
async def get_task_status(task_id: uuid.UUID) -> TaskStatus:
    eng = await get_engine()
    async with eng.connect() as conn:
        row = await get_task(conn, task_id)
        if not row:
            raise HTTPException(404, "task not found")
        return TaskStatus(id=task_id, status=row.status, model_used=row.model_used, latency_ms=row.latency_ms, template_ver=row.template_ver)

@router.post("/v1/tasks/{task_id}/cancel", dependencies=[Depends(require_api_key)])
async def cancel_task(task_id: uuid.UUID, request: Request, x_api_key: str | None = Header(None)):
    _ratelimit_guard(x_api_key)
    eng = await get_engine()
    async with eng.begin() as conn:
        await update_task_status(conn, task_id, "canceled")
    # cancel inflight work as well
    q = getattr(request.app.state, 'job_queue', None)
    if q is not None:
        await q.cancel(str(task_id))
    await hub.publish(str(task_id), json.dumps({"status":"canceled"}))
    return {"ok": True}

@router.post("/v1/feedback", dependencies=[Depends(require_api_key)])
async def submit_feedback(feedback: FeedbackV1, x_api_key: str | None = Header(None)):
    _ratelimit_guard(x_api_key)
    eng = await get_engine()
    async with eng.begin() as conn:
        await conn.execute(text("""
            INSERT INTO rewards(id, task_id, model, success, latency_ms, human_score)
            VALUES (gen_random_uuid(), :task_id, :model, :success, :latency_ms, :human_score)
        """), dict(task_id=str(feedback.task_id), model=feedback.model, success=feedback.success,
                   latency_ms=feedback.latency_ms, human_score=feedback.human_score))
        bonus = (feedback.human_score or 0) * 0.02
        await conn.execute(text("""
            INSERT INTO bandit_stats(model, feature_hash, runs, reward_sum, reward_sq_sum, last_updated)
            VALUES (:model, :fh, 1, :r, :r2, now())
            ON CONFLICT (model, feature_hash)
            DO UPDATE SET
                runs = bandit_stats.runs + 1,
                reward_sum = bandit_stats.reward_sum + EXCLUDED.reward_sum,
                reward_sq_sum = bandit_stats.reward_sq_sum + EXCLUDED.reward_sq_sum,
                last_updated = now()
        """), dict(model=feedback.model, fh="manual", r=(1.0 if feedback.success else 0.0)+bonus, r2=((1.0 if feedback.success else 0.0)+bonus)**2))
    return {"ok": True}

@router.get("/v1/stream/{task_id}")
async def stream_task(task_id: uuid.UUID):
    async def event_gen():
        async for chunk in hub.stream(str(task_id), heartbeat_seconds=10):
            yield chunk
    return StreamingResponse(event_gen(), media_type="text/event-stream")
# --- UI fallback under /v1 for convenience ---
try:
    from app.ui import DASHBOARD_HTML
    @router.get("/ui", include_in_schema=False, response_class=HTMLResponse)
    def ui_fallback():
        return HTMLResponse(content=DASHBOARD_HTML)
except Exception:
    pass
